{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a reproduction and introduction to the following scholarly article:   \n",
    "\n",
    "@article{hansen2021frontiers,  \n",
    "  title={**Frontiers: Algorithmic collusion: Supra-competitive prices via independent algorithms**},  \n",
    "  author={Hansen, Karsten T and Misra, Kanishka and Pai, Mallesh M},  \n",
    "  journal={**Marketing Science**},  \n",
    "  volume={40},  \n",
    "  number={1},  \n",
    "  pages={1--12},  \n",
    "  year={2021},  \n",
    "  publisher={INFORMS}  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL, DR**:\n",
    "\n",
    "This paper analysis the outcomes when two competing sllers (duopoly markets) use UCB algorithm to price homogeneious product dynamically. \n",
    "\n",
    "The profit function for each firm is stochastic: $\\pi_i(p_i, p_{-i}, \\delta)$, where $\\delta$ is clled the signal-to-noise ration (**SNR**). A higher $\\delta$ indicates lower stochasticity in profit.\n",
    "\n",
    "The system dynamics yield two possible outcomes:\n",
    "\n",
    "1. Convergence to Nash equilibrum;\n",
    "\n",
    "2. Convergence to a **supra-competitive** scenario resembling monopoly pricing.\n",
    "\n",
    "The learning algorithm is misspecified,  as the UCB algorithm neglects consideration of the competitor's decision.\n",
    "\n",
    "However, the numerical experiments reveal that the outcome depends on the SNR:\n",
    "\n",
    "1. when SNR is low: end up with the Nash-equilibrum;\n",
    "\n",
    "2. when SNR is high: end up with the supra-competitive scenario.\n",
    "\n",
    "The core of this paper consists of numerical expirements, complemented by straightforward mathematical proofs in the appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(93)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Model Setting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume there are 2 firms (denoted by $i, j$) selling a homogeneous product within an infinte time period. At each period $t$, firm $i$ gives price $p_{i,t}$ from a discrete space $P=\\{p_1, p_2, \\cdots, p_K\\}$. After facing the price $p_{-i,t}$ by the competitor, the profit can be obtained through a parametric data-generating process (DGP):\n",
    "\n",
    "$$\n",
    "\\pi_{i,t} = (\\alpha - \\beta p_{i,t} + \\gamma p{-i, t})\\cdot p_{i,t} + \\epsilon_t, \\\\\n",
    "\\epsilon_t = \\sim U[-\\frac1\\delta,\\frac1\\delta],\n",
    "$$\n",
    "where $\\delta$ is called the signal-to-noise ration (**SNR**).\n",
    "\n",
    "From the DGP we can observe:\n",
    "\n",
    "1. The marginal cost of the product is zero;\n",
    "2. The demand function is deterministic, the random term is added to the profit only;\n",
    "3. The SNR is inversely proportional to the stability of the profit;\n",
    "4. The demand function is symmetric for both firms.\n",
    "\n",
    "Under such settings, we can calculate the prices under Nash equilibrium (the competitive case) or under collusive pricing (the supra-competitive case) analytically:\n",
    "\n",
    "$$\n",
    "Collusive: p^D = \\frac{\\alpha}{2\\beta-\\gamma} \\\\\n",
    "Collusive: p^M = \\frac{\\alpha}{2(\\beta-\\gamma)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hansen_env():\n",
    "\n",
    "    def __init__(self, info):\n",
    "        \"\"\"\n",
    "        The input parameters info should be dictionary.\n",
    "        info should contain keys: 'alpha', 'beta', 'gamma', 'delta'\n",
    "        \"\"\"\n",
    "        self.info = info\n",
    "        self.rewards_log = []\n",
    "        self.actions_log = []\n",
    "        self.prices_log = []\n",
    "        self.action_spaces = None\n",
    "        self.nash = None\n",
    "        self.collusion = None\n",
    "        self.check()\n",
    "        self.initialize()\n",
    "    \n",
    "    def check(self):\n",
    "        keys = ['alpha', 'beta', 'gamma', 'delta']\n",
    "        if not all(key in self.info for key in keys):\n",
    "            raise ValueError(\"Incorrect Parameters\")\n",
    "        \n",
    "    def step(self, actions):\n",
    "        self.actions_log.append(actions)\n",
    "        prices = self.action_spaces[0][actions[0]], self.action_spaces[1][actions[1]]\n",
    "        self.prices_log.append(prices)\n",
    "        rewards = self.execute(prices)\n",
    "        self.rewards_log.append(rewards)\n",
    "        return rewards\n",
    "    \n",
    "    def execute(self, prices):\n",
    "        profit1 = (prices[0] * (self.info['alpha']-self.info['beta']*prices[0]+self.info['gamma']*prices[1])) + np.random.uniform(-1/self.info['delta'], 1/self.info['delta'])\n",
    "        profit2 = (prices[1] * (self.info['alpha']-self.info['beta']*prices[1]+self.info['gamma']*prices[0])) + np.random.uniform(-1/self.info['delta'], 1/self.info['delta'])\n",
    "        return (profit1, profit2)\n",
    "    \n",
    "    def initialize(self, lower = 1, upper = 2.5, space = 0.01):\n",
    "        \"\"\"\n",
    "        Compute the nash, collusion prices and the actions spaces.\n",
    "        \"\"\"\n",
    "        nash = self.info['alpha'] / (2*self.info['beta']-self.info['gamma']) \n",
    "        self.nash = np.array([nash, nash])\n",
    "        collusion = 0.5 * self.info['alpha'] / (self.info['beta']-self.info['gamma']) \n",
    "        self.collusion = np.array([collusion, collusion])\n",
    "        self.action_spaces = [np.array([nash, collusion]), np.array([nash, collusion])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline model, the action space for each agent is $2$, meaning each agent can only pick price from the $p^D$ and $p^M$. \n",
    "\n",
    "The agent utilizes $UCB-tuned$ algorithm to make decision at time $t$:\n",
    "\n",
    "\\begin{aligned}\n",
    "V_{k,t}& =\\overline{\\pi_{k,t}^{2}}-\\bar{\\pi}_{k,t}^{2}+\\sqrt{\\frac{2\\log t}{n_{k,t}}},  \\\\\n",
    "\\mathrm{UCB-tuned}_{k,t}& =\\bar{\\pi}_{k,t}+\\sqrt{\\frac{\\log t}{n_{k,t}}\\mathrm{min}\\left(\\frac14,\\mathrm{V}_{k,t}\\right),} \n",
    "\\end{aligned}\n",
    "\n",
    "where \\(n_{k, t}\\) represents the frequency of the agent taking action \\(k\\) up to time \\(t\\).To give an intuitive understanding on the formula:\n",
    "\n",
    "1. The term $\\overline{\\pi_{k,t}^{2}}-\\bar{\\pi}_{k,t}^{2}$ is the empirical variance.\n",
    "\n",
    "2. The term $\\sqrt{\\frac{\\log t}{n_{k,t}}\\mathrm{min}(\\frac14,\\mathrm{V}_{k,t})}$ is used for exploratin.\n",
    "\n",
    "The agent pick action $k$ at time $t$ with the highest index $\\mathrm{UCB-tuned}_{k,t}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hansen_agent:\n",
    "    def __init__(self):\n",
    "        self.t = 0\n",
    "        self.n = np.array([0, 0])\n",
    "        self.last_action = None\n",
    "        self.reward = [[], []]\n",
    "\n",
    "    def action(self):\n",
    "        pi = np.array((np.mean(self.reward[0]), np.mean(self.reward[1])))\n",
    "        Pi = np.array((np.mean(np.array(self.reward[0]) ** 2), np.mean(np.array(self.reward[1]) ** 2)))\n",
    "        temp = np.sqrt(2 * np.log(self.t) / self.n)\n",
    "        V = Pi - pi**2 + temp\n",
    "        temp = np.log(self.t) / self.n\n",
    "        temp *= np.minimum(0.25, V)\n",
    "        UCB = pi + np.sqrt(temp)\n",
    "        action = np.argmax(UCB)\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.t += 1\n",
    "        self.n[self.last_action] += 1\n",
    "        self.reward[self.last_action].append(reward)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the agents have no information on the bandit at the beginning of the game, they explore randomly at the first two rounds:\n",
    "\n",
    "1. In the first round, each agent randomly selects one action.\n",
    "2. In the second round, each agent chooses the opposite action to that of the first round (there are 2 actins in total).\n",
    "3. Subsequently, each agent makes decisions using the UCB-tuned algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(env, agent1, agent2):\n",
    "    action1 = np.random.choice(np.array(2), 2)\n",
    "    agent1.last_action, agent2.last_action = action1\n",
    "    rewards = env.step((agent1.last_action, agent2.last_action))\n",
    "    agent1.update(rewards[0])\n",
    "    agent2.update(rewards[1])\n",
    "    agent1.last_action, agent2.last_action = 1 - action1\n",
    "    rewards = env.step((agent1.last_action, agent2.last_action))\n",
    "    agent1.update(rewards[0])\n",
    "    agent2.update(rewards[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the parameters set in the paper to construct an environment for interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'alpha': 0.48, 'beta': 0.9, 'gamma': 0.6, 'delta': 0.1}\n",
    "env = Hansen_env(info)\n",
    "agent1, agent2 = Hansen_agent(), Hansen_agent()\n",
    "initialize(env, agent1, agent2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The Main Resuts\n",
    "\n",
    "In the paper, the authors dicuss the impact of *SNR* on the pricing outcome. \n",
    "\n",
    "For each given SNR, 500 Monte Carlo Simulations are performed. For each simulation, we consider the **median price** charged in the last 1,000 rounds out of two million rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m initialize(env, agent1, agent2)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2000000\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     actions \u001b[38;5;241m=\u001b[39m (agent1\u001b[38;5;241m.\u001b[39maction(), \u001b[43magent2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     13\u001b[0m     agent1\u001b[38;5;241m.\u001b[39mupdate(rewards[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mHansen_agent.action\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maction\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      9\u001b[0m     pi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray((np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward[\u001b[38;5;241m0\u001b[39m]), np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward[\u001b[38;5;241m1\u001b[39m])))\n\u001b[1;32m---> 10\u001b[0m     Pi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray((np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m), np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m     11\u001b[0m     temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m     12\u001b[0m     V \u001b[38;5;241m=\u001b[39m Pi \u001b[38;5;241m-\u001b[39m pi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m temp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delta_list = [0.1, 0.2, 0.4, 1, 2.5, 5, 10]\n",
    "results = {delta:[] for delta in delta_list}\n",
    "for delta in delta_list:\n",
    "    print(delta)\n",
    "    for _ in range(500):\n",
    "        info = {'alpha': 0.48, 'beta': 0.9, 'gamma': 0.6, 'delta': 0.1}\n",
    "        env = Hansen_env(info)\n",
    "        agent1, agent2 = Hansen_agent(), Hansen_agent()\n",
    "        initialize(env, agent1, agent2)\n",
    "        for __ in range(2000000 - 2):\n",
    "            actions = (agent1.action(), agent2.action())\n",
    "            rewards = env.step(actions)\n",
    "            agent1.update(rewards[0])\n",
    "            agent2.update(rewards[1])\n",
    "        prices = np.array(env.prices_log)[-1000:, :]\n",
    "        price1, price2 = np.median(prices[:, 0]), np.median(prices[:, 1])\n",
    "        results[delta].append([price1, price2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for __ in range(100):\n",
    "    actions = (agent1.action(), agent2.action())\n",
    "    rewards = env.step(actions)\n",
    "    agent1.update(rewards[0])\n",
    "    agent2.update(rewards[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = np.array(env.prices_log)[-1000:, :]\n",
    "price1, price2 = np.median(prices[:, 0]), np.median(prices[:, 1])\n",
    "results[delta].append([price1, price2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7999999999999998, 0.3999999999999999]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[delta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m results[delta]\n\u001b[1;32m----> 2\u001b[0m sns\u001b[38;5;241m.\u001b[39mkdeplot(x\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, y\u001b[38;5;241m=\u001b[39mdata[:, \u001b[38;5;241m1\u001b[39m], fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m\"\u001b[39m, cbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2D Density Plot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "data = results[delta]\n",
    "sns.kdeplot(x=data[:, 0], y=data[:, 1], fill=True, cmap=\"Blues\", cbar=True)\n",
    "plt.title(\"2D Density Plot\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Robust Tests & Proof"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
